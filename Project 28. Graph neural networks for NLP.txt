Description:
Graphs are natural representations for many NLP tasks â€” such as dependency parsing, semantic graphs, or knowledge graphs. With Graph Neural Networks, we can model the relationships between words, phrases, or entities more effectively. In this project, weâ€™ll use a GCN to classify sentences based on their dependency parse trees.

ðŸ§ª Python Implementation (Sentence Classification via Dependency Graph GCN)
We'll simulate dependency graphs using spaCy and classify simple sentences as positive or negative sentiment.

âœ… Required Install:
pip install spacy torch torch-geometric
python -m spacy download en_core_web_sm
ðŸš€ Code:
import torch
import torch.nn.functional as F
import spacy
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
 
# 1. Prepare NLP model
nlp = spacy.load("en_core_web_sm")
 
# 2. Create dependency graph from sentence
def sentence_to_graph(sentence):
    doc = nlp(sentence)
    edge_index = []
    x = []
 
    for token in doc:
        x.append(token.vector if token.has_vector else torch.rand(300))  # use random if no vector
        if token.i != token.head.i:  # skip root
            edge_index.append([token.i, token.head.i])
            edge_index.append([token.head.i, token.i])  # bidirectional
 
    x = torch.tensor(x, dtype=torch.float)
    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
    return Data(x=x, edge_index=edge_index)
 
# 3. Define GCN model for sentence graph
class SentenceGCN(torch.nn.Module):
    def __init__(self, in_dim, hidden_dim):
        super().__init__()
        self.conv1 = GCNConv(in_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.classifier = torch.nn.Linear(hidden_dim, 2)
 
    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = x.mean(dim=0)  # mean pooling over sentence
        return F.log_softmax(self.classifier(x), dim=0)
 
# 4. Sample data
sentences = [
    ("I love this movie!", 1),
    ("This film is terrible.", 0),
    ("What an amazing experience.", 1),
    ("I wouldn't recommend it.", 0)
]
 
# 5. Training setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = SentenceGCN(in_dim=300, hidden_dim=64).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
loss_fn = torch.nn.NLLLoss()
 
# 6. Training loop
for epoch in range(1, 51):
    model.train()
    total_loss = 0
    for text, label in sentences:
        data = sentence_to_graph(text).to(device)
        optimizer.zero_grad()
        out = model(data.x, data.edge_index)
        loss = loss_fn(out.unsqueeze(0), torch.tensor([label], dtype=torch.long, device=device))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    if epoch % 10 == 0:
        print(f"Epoch {epoch:02d}, Loss: {total_loss:.4f}")
âœ… What It Does:
Parses sentences into dependency trees using spaCy.

Constructs graphs where words are nodes, and edges are syntactic dependencies.

Applies GCN to classify sentence sentiment.

Uses mean pooling over word representations for graph-level prediction.
-